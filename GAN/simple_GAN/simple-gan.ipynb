{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter  # to print to tensorboard","metadata":{"execution":{"iopub.status.busy":"2023-07-23T22:01:18.522388Z","iopub.execute_input":"2023-07-23T22:01:18.522744Z","iopub.status.idle":"2023-07-23T22:01:29.293451Z","shell.execute_reply.started":"2023-07-23T22:01:18.522714Z","shell.execute_reply":"2023-07-23T22:01:29.292401Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        self.disc = nn.Sequential(\n            nn.Linear(in_features, 128),\n            nn.LeakyReLU(0.01),\n            nn.Linear(128, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.disc(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T22:01:29.299518Z","iopub.execute_input":"2023-07-23T22:01:29.302549Z","iopub.status.idle":"2023-07-23T22:01:29.312533Z","shell.execute_reply.started":"2023-07-23T22:01:29.302513Z","shell.execute_reply":"2023-07-23T22:01:29.311289Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, z_dim, img_dim):\n        super().__init__()\n        self.gen = nn.Sequential(\n            nn.Linear(z_dim, 256),\n            nn.LeakyReLU(0.01),\n            nn.Linear(256, img_dim),\n            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n        )\n\n    def forward(self, x):\n        return self.gen(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T22:01:29.313891Z","iopub.execute_input":"2023-07-23T22:01:29.314384Z","iopub.status.idle":"2023-07-23T22:01:29.328987Z","shell.execute_reply.started":"2023-07-23T22:01:29.314352Z","shell.execute_reply":"2023-07-23T22:01:29.327839Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters etc.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlr = 3e-4\nz_dim = 64\nimage_dim = 28 * 28 * 1  # 784\nbatch_size = 32\nnum_epochs = 50\n\ndisc = Discriminator(image_dim).to(device)\ngen = Generator(z_dim, image_dim).to(device)\nfixed_noise = torch.randn((batch_size, z_dim)).to(device)\ntransforms = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T22:01:29.331509Z","iopub.execute_input":"2023-07-23T22:01:29.331843Z","iopub.status.idle":"2023-07-23T22:01:34.827979Z","shell.execute_reply.started":"2023-07-23T22:01:29.331811Z","shell.execute_reply":"2023-07-23T22:01:34.826845Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\nloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\nopt_disc = optim.Adam(disc.parameters(), lr=lr)\nopt_gen = optim.Adam(gen.parameters(), lr=lr)\ncriterion = nn.BCELoss()\nwriter_fake = SummaryWriter(f\"logs/fake\")\nwriter_real = SummaryWriter(f\"logs/real\")\nstep = 0","metadata":{"execution":{"iopub.status.busy":"2023-07-23T22:01:34.829487Z","iopub.execute_input":"2023-07-23T22:01:34.829843Z","iopub.status.idle":"2023-07-23T22:01:35.936972Z","shell.execute_reply.started":"2023-07-23T22:01:34.829806Z","shell.execute_reply":"2023-07-23T22:01:35.936023Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 90188467.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 77800702.52it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1648877/1648877 [00:00<00:00, 24923298.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 13183756.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    for batch_idx, (real, _) in enumerate(loader):\n        real = real.view(-1, 784).to(device)\n        batch_size = real.shape[0]\n\n        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n        noise = torch.randn(batch_size, z_dim).to(device)\n        fake = gen(noise)\n        disc_real = disc(real).view(-1)\n        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n        disc_fake = disc(fake).view(-1)\n        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n        lossD = (lossD_real + lossD_fake) / 2\n        disc.zero_grad()\n        lossD.backward(retain_graph=True)\n        opt_disc.step()\n\n        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n        # where the second option of maximizing doesn't suffer from\n        # saturating gradients\n        output = disc(fake).view(-1)\n        lossG = criterion(output, torch.ones_like(output))\n        gen.zero_grad()\n        lossG.backward()\n        opt_gen.step()\n\n        if batch_idx == 0:\n            print(\n                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n            )\n\n            with torch.no_grad():\n                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n                data = real.reshape(-1, 1, 28, 28)\n                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n\n                writer_fake.add_image(\n                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n                )\n                writer_real.add_image(\n                    \"Mnist Real Images\", img_grid_real, global_step=step\n                )\n                step += 1","metadata":{"execution":{"iopub.status.busy":"2023-07-23T22:01:39.637892Z","iopub.execute_input":"2023-07-23T22:01:39.638275Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch [0/50] Batch 0/1875                       Loss D: 0.7020, loss G: 0.7355\nEpoch [1/50] Batch 0/1875                       Loss D: 0.2792, loss G: 1.7540\nEpoch [2/50] Batch 0/1875                       Loss D: 0.4366, loss G: 1.2836\nEpoch [3/50] Batch 0/1875                       Loss D: 0.8271, loss G: 0.6203\nEpoch [4/50] Batch 0/1875                       Loss D: 0.6441, loss G: 0.8311\nEpoch [5/50] Batch 0/1875                       Loss D: 0.4704, loss G: 1.1930\nEpoch [6/50] Batch 0/1875                       Loss D: 0.9135, loss G: 0.6401\nEpoch [7/50] Batch 0/1875                       Loss D: 0.5405, loss G: 1.1117\nEpoch [8/50] Batch 0/1875                       Loss D: 0.6723, loss G: 0.8476\nEpoch [9/50] Batch 0/1875                       Loss D: 0.5320, loss G: 1.2646\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}